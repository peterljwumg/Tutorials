str_replace_all("Hello My Friend", " ", "+")
name <- "3 reasons retirees are flocking to this desert state"
name <- str_replace_all(name, " ", "+")
url <- paste0("https://www.google.com/search?q=msn+", name)
google_search <- read_html(url)
google_search %>%
html_nodes(".r") %>%
html_attr('href')
name <- "3 reasons retirees are flocking to this desert state"
name <- str_replace_all(name, " ", "+")
url <- paste0("https://www.google.com/search?q=msn+", name)
google_search <- read_html(url)
url
google_search %>%
html_nodes(".iUh30")
x <- google_search %>%
html_nodes(".iUh30")
View(x)
View(google_search)
View(google_search)
View(google_search)
x <- google_search %>%
html_nodes(".iUh30")
google_search <- read_html("https://stackoverflow.com/questions/35247033/using-rvest-to-extract-links")
x <- google_search %>%
html_nodes(".post-text")
View(x)
name <- "3 reasons retirees are flocking to this desert state"
name <- str_replace_all(name, " ", "+")
url <- paste0("https://www.google.com/search?q=msn+", name)
google_search <- read_html(url)
x <- google_search %>%
html_nodes(".iUh30")
View(google_search)
name <- "msn 3 reasons retirees are flocking to this desert state"
name <- str_replace_all(name, " ", "+")
url <- paste0("https://www.google.com/search?q=", name)
google_search <- read_html(url)
google_search <- read_html("https://www.google.com/search?q=curry")
x <- google_search %>%
html_nodes(".st")
url
library(rvest)
library(tidyverse)
name <- "3 reasons retirees are flocking to this desert state"
name <- str_replace_all(name, " ", "+")
url <- paste0("https://www.google.com/search?q=msn+", name, "&as_sitesearch=msn.com")
google_search <- read_html(url)
url
google_search
install.packages("splashr")
library(splashr)
install_splash()
sp <- start_splash()
install_splash()
install_splash()
library(rvest)
library(splashr)
library(tidyverse)
install_splash()
install.packages(c("curl", "httr"))
install.packages(c("curl", "httr"))
library(rvest)
library(splashr)
library(tidyverse)
sp <- start_splash()
library(httr)
set_config(use_proxy(url="10.3.100.207",port=8080))
install_splash()
library(splashr)
install_splash()
install_splash()
library(splashr)
install_splash()
devtools::install_github("wch/harbor")
devtools::install_github("hrbrmstr/splashr")
library(splashr)
install_splash()
library(splashr)
install_splash()
install_splash()
page <- read_html("https://www.msn.com/en-us/money/retirement/3-reasons-retirees-are-flocking-to-this-desert-state/ar-BBWbEg1")
library(rvest)
library(splashr)
library(tidyverse)
page <- read_html("https://www.msn.com/en-us/money/retirement/3-reasons-retirees-are-flocking-to-this-desert-state/ar-BBWbEg1")
page %>%
html_nodes("h1")
View(page)
page %>%
html_name("script")
xml_attrs(xml_child(xml_child(page, 1), 12))[["src"]]
page %>%
html_nodes(xpath = "/html/head/link[12]")
page %>%
html_nodes(xpath = "//html/head/link[12]")
page <- read_html("https://www.msn.com/en-us/money/retirement/3-reasons-retirees-are-flocking-to-this-desert-state/ar-BBWbEg1")
page %>%
html_nodes(xpath = "/html/head/link[12]")
x <- page %>%
html_nodes(xpath = "/html/head/link[12]")
View(x)
View(page)
xml_attrs(xml_child(xml_child(page, 1), 6))
x <- page %>%
xml_attrs(xml_child(xml_child(page, 1), 6))
pag[xml_attrs(xml_child(xml_child(page, 1), 6))]
page[xml_attrs(xml_child(xml_child(page, 1), 6))]
xml_attrs(xml_child(xml_child(page, 1), 6))
xml_child(xml_child(page, 1), 6)
xml_child(xml_child(page, 1), 6)
xml_attrs(xml_child(xml_child(page, 1), 6))[["href"]]
library(readr)
library(tidyverse)
library(text2vec)
library(textcat)
#################### preprocessing ####################
# load data
dataset <- as_tibble(read_csv("title_pairs.csv"))
# filter English only titles
dataset <- dataset %>%
filter(textcat(title2) != "spanish")
titleA <- dataset$title1
titleB <- dataset$title2
# text preprocessing function
prep_fun <- function(x) {
x %>%
# make text lower case
str_to_lower %>%
# remove non-alphanumeric symbols
str_replace_all("[^[:alnum:]]", " ") %>%
# collapse multiple spaces
str_replace_all("\\s+", " ")
}
# preproess texts
titleA_clean <- prep_fun(titleA)
titleB_clean <- prep_fun(titleB)
# remove exact match
sum(titleA_clean == titleB_clean)
12/7/(12/7+19.2)
16/7/(16/7+76.8)
3/7-0.2
qnorm(0.975)
p1 <- 421 / 2000
q1 <- 1 - p1
p2 <- 430 / 1000
q2 <- 1 - p2
p1 - qnorm(0.975) * sqrt(p1 * q1 / 2000)
p1 + qnorm(0.975) * sqrt(p1 * q1 / 2000)
p2 - qnorm(0.975) * sqrt(p2 * q2 / 1000)
p2 + qnorm(0.975) * sqrt(q2 * p2 / 1000)
(p1 - qnorm(0.975) * sqrt(p1 * q1 / 2000) - 0.15) / 0.77
(p1 + qnorm(0.975) * sqrt(p1 * q1 / 2000) - 0.15) / 0.77
(p2 - qnorm(0.975) * sqrt(p2 * q2 / 1000) - 0.35) / 0.57
(p2 + qnorm(0.975) * sqrt(q2 * p2 / 1000) - 0.35) / 0.57
(p1 - 0.15) / 0.77 - (p2 - 0.35) / 0.57
sqrt(p1 * q1 / 2000 + q2 * p2 / 1000)
((p1 - 0.15) / 0.77 - (p2 - 0.35) / 0.57) / sqrt(p1 * q1 / 2000 + q2 * p2 / 1000)
library(ISLR)
attach(Wage)
# Polynomial Regression and Step Functions
fit=lm(wage~poly(age,4),data=Wage)
coef(summary(fit))
?poly
fit=lm(wage~poly(age,4),data=Wage)
coef(summary(fit))
fit2=lm(wage~poly(age,4,raw=T),data=Wage)
coef(summary(fit2))
fit2a=lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
coef(fit2a)
fit2b=lm(wage~cbind(age,age^2,age^3,age^4),data=Wage)
agelims=range(age)
age.grid=seq(from=agelims[1],to=agelims[2])
preds=predict(fit,newdata=list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
par(mfrow=c(1,2),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Degree-4 Polynomial",outer=T)
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
preds2=predict(fit2,newdata=list(age=age.grid),se=TRUE)
max(abs(preds$fit-preds2$fit))
fit.1=lm(wage~age,data=Wage)
fit.2=lm(wage~poly(age,2),data=Wage)
fit.3=lm(wage~poly(age,3),data=Wage)
fit.4=lm(wage~poly(age,4),data=Wage)
fit.5=lm(wage~poly(age,5),data=Wage)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
coef(summary(fit.5))
(-11.983)^2
fit.1=lm(wage~education+age,data=Wage)
fit.2=lm(wage~education+poly(age,2),data=Wage)
fit.3=lm(wage~education+poly(age,3),data=Wage)
anova(fit.1,fit.2,fit.3)
fit=glm(I(wage>250)~poly(age,4),data=Wage,family=binomial)
preds=predict(fit,newdata=list(age=age.grid),se=T)
pfit=exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit = cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit))
preds=predict(fit,newdata=list(age=age.grid),type="response",se=T)
plot(age,I(wage>250),xlim=agelims,type="n",ylim=c(0,.2))
points(jitter(age), I((wage>250)/5),cex=.5,pch="|",col="darkgrey")
lines(age.grid,pfit,lwd=2, col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
table(cut(age,4))
fit=lm(wage~cut(age,4),data=Wage)
coef(summary(fit))
library(splines)
fit=lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
pred=predict(fit,newdata=list(age=age.grid),se=T)
plot(age,wage,col="gray")
lines(age.grid,pred$fit,lwd=2)
lines(age.grid,pred$fit+2*pred$se,lty="dashed")
lines(age.grid,pred$fit-2*pred$se,lty="dashed")
dim(bs(age,knots=c(25,40,60)))
dim(bs(age,df=6))
attr(bs(age,df=6),"knots")
fit2=lm(wage~ns(age,df=4),data=Wage)
pred2=predict(fit2,newdata=list(age=age.grid),se=T)
lines(age.grid, pred2$fit,col="red",lwd=2)
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Smoothing Spline")
fit=smooth.spline(age,wage,df=16)
fit2=smooth.spline(age,wage,cv=TRUE)
fit2$df
lines(fit,col="red",lwd=2)
lines(fit2,col="blue",lwd=2)
legend("topright",legend=c("16 DF","6.8 DF"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Local Regression")
fit=loess(wage~age,span=.2,data=Wage)
fit2=loess(wage~age,span=.5,data=Wage)
lines(age.grid,predict(fit,data.frame(age=age.grid)),col="red",lwd=2)
lines(age.grid,predict(fit2,data.frame(age=age.grid)),col="blue",lwd=2)
legend("topright",legend=c("Span=0.2","Span=0.5"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
library(gam)
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
par(mfrow=c(1,3))
plot(gam.m3, se=TRUE,col="blue")
plot.gam(gam1, se=TRUE, col="red")
gam.m1=gam(wage~s(age,5)+education,data=Wage)
gam.m2=gam(wage~year+s(age,5)+education,data=Wage)
anova(gam.m1,gam.m2,gam.m3,test="F")
summary(gam.m3)
preds=predict(gam.m2,newdata=Wage)
gam.lo=gam(wage~s(year,df=4)+lo(age,span=0.7)+education,data=Wage)
plot.gam(gam.lo, se=TRUE, col="green")
gam.lo.i=gam(wage~lo(year,age,span=0.5)+education,data=Wage)
library(akima)
plot(gam.lo.i)
gam.lr=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="green")
table(education,I(wage>250))
gam.lr.s=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage,subset=(education!="1. < HS Grad"))
plot(gam.lr.s,se=T,col="green")
install.packages('gam')
library(gam)
fit1=lm(wage~age+education,data=Wage)
fit2=lm(wage~poly(age,2)+education,data=Wage)
fit3=lm(wage~poly(age,3)+education,data=Wage)
anova(fit1,fit2,fit3,test="F")
# Question 2
summary(fit3)
table(cut(age,4))
?cut
table(cut(age,5))
table(cut(age,breaks = c(0, 25, 35, 45, 55, 80)))
fit=lm(wage~cut(age,breaks = c(0, 25, 35, 45, 55, 80)),data=Wage)
coef(summary(fit))
predict(fit,newdata=list(age=35),se=TRUE)
library(splines)
fit=lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
pred=predict(fit,newdata=list(age=55))
fit2=lm(wage~ns(age,df=4),data=Wage)
pred=predict(fit2,newdata=list(age=55))
fit3=smooth.spline(age,wage,cv=F)
pred=predict(fit3,newdata=list(age=55))
fit4=loess(wage~age,span=.5,data=Wage)
pred=predict(fit4,newdata=list(age=55))
library(splines)
fit=lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
predict(fit,newdata=list(age=55))
fit2=lm(wage~ns(age,df=4),data=Wage)
predict(fit2,newdata=list(age=55))
fit3=smooth.spline(age,wage,cv=F)
predict(fit3,newdata=list(age=55))
fit4=loess(wage~age,span=.5,data=Wage)
predict(fit4,newdata=list(age=55))
fit=lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
predict(fit,newdata=list(age=55))
predict(fit2,newdata=list(age=55))
predict(fit3,newdata=list(age=55))
predict(fit4,newdata=list(age=55))
fit4=loess(wage~age,span=.5,data=Wage)
predict(fit4,newdata=list(age=55))
fit=lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
predict(fit,newdata=list(age=55))
fit2=lm(wage~ns(age,df=4),data=Wage)
predict(fit2,newdata=list(age=55))
fit3=smooth.spline(age,wage,cv=F)
predict(fit3,newdata=list(age=55))
# Question 5
gam.m=gam(wage~year+s(age,5)+education,data=Wage)
coefficients(gam.m)
table(Wage$education)
Wage$education
# Question 6
predict(gam.m,newdata=list(age=48,year=2008,education='5. Advanced Degree'))
# Question 7
gam.m=gam(wage~year+s(age,3)+education,data=Wage)
predict(gam.m,newdata=list(age=48,year=2008,education='5. Advanced Degree'))
library(tidyverse)
library(e1071)
library(caret)
library(quanteda)
library(irlba)
library(randomForest)
library(doSNOW)
# ----------------- Part 1: Intro + Objective -----------------
# load data
data.raw = read.csv("../Data/spam.csv", stringsAsFactors = F, fileEncoding="latin1")
# data preprocessing
data.raw = data.raw[1:2] # keep only the useful columns
colnames(data.raw) = c("Label", "Text") # rename columns
data.raw$Label = as.factor(data.raw$Label)
# check if there are missing values
length(which(!complete.cases(data.raw)))
# distribution of label
prop.table(table(data.raw$Label))
# count the length of each text
data.raw$TextLength = nchar(data.raw$Text)
summary(data.raw$TextLength)
# visualize text length distribution
ggplot(data.raw, aes(x = TextLength, fill = Label)) +
theme_bw() +
geom_histogram(bins = 80) +
labs(y = "Text Count", x = "Length of Text",
title = "Distribution of Text Lengths")
# summary statistics
data.raw %>%
group_by(Label) %>%
summarise("mean" = mean(TextLength),
"median" = median(TextLength),
"sd" = sd(TextLength))
# t-test
with(data.raw, shapiro.test(TextLength[Label == "ham"]))
with(data.raw, shapiro.test(TextLength[Label == "spam"]))
res.ftest <- var.test(TextLength ~ Label, data = data.raw)
res.ftest
res <- t.test(data.raw$TextLength[data.raw$Label=="ham"],
data.raw$TextLength[data.raw$Label=="spam"], var.equal = FALSE)
# conclusion: text length may help with determining spam/ham
# ----------------- Part 2: Train/Test Split -----------------
# train/test split (70%/30% stratified split)
set.seed(32984)
indicies = createDataPartition(data.raw$Label, times = 1, p = 0.7, list = F)
train.data = data.raw[indicies,]
test.data = data.raw[-indicies,]
# verifying that'createDataPartition' preserved the original proportions of labels
prop.table(table(train.data$Label))
prop.table(table(test.data$Label))
# About BOW or DFM:
# BOW(bag-of-words) or DFM(docuemnt frequency matrix) is a common model to convert
# text into dataframes. Tokenize words into columns and let documents correspond
# to rows, where the cells' values represent the frequency of the token in the document
# Considerations: casing, punctuation, numbers, every word, symbols, similar words
# Thus, pre-processing is crucial for text analytics!
# ----------------- Part 3: Preproessing Pipeline + DFM -----------------
# exploring some issues to be handled with pre-processing
train.data$Text[21]
train.data$Text[38]
train.data$Text[357]
# tokenize SMS text messages
train.tokens = tokens(train.data$Text, what = "word",
remove_numbers = T, remove_punct = T,
remove_symbols = T, remove_hyphens = T)
# see examples of the result
train.tokens[[300]]
train.tokens[[357]]
# lower case all tokens
train.tokens = tokens_tolower(train.tokens)
train.tokens[[357]]
# use quanteda's built-in stopword list for English
# NOTE: always inspect stopword lists for applicability to the problem/domain
train.tokens = tokens_select(train.tokens, stopwords(), selection = "remove")
# see examples of the result
train.tokens[[357]]
# perform stemming on the tokens
train.tokens = tokens_wordstem(train.tokens, language = "english")
train.tokens[[357]]
# create a BOW model
train.tokens.dfm = dfm(train.tokens, tolower = F)
train.tokens.df = as.data.frame(train.tokens.dfm)
dim(train.tokens.df)
view(train.tokens.df[1:10, 1:100]) # problem: sparsity & curse of dimensionality
colnames(train.tokens.df[1:30])
# ----------------- Part 4: BOW Model -----------------
# Per best practice, we will leverage cross validation as the basis of the modeling process
# set up a feature data frame with labels
train.tokens.data = cbind(Label = train.data$Label,
train.tokens.df)
# colnames need some preprocessing
names(train.tokens.data[c(146, 148, 235, 238)])
# cleanup column names in an automatic fashion
names(train.tokens.data) = make.names(names(train.tokens.data))
# use caret to create stratified folds for 10-fold CV repeated 3 times
set.seed(48743)
cv.folds = createMultiFolds(train.data$Label, k = 10, times = 3)
cv.cntrl = trainControl(method = "repeatedcv", number = 10,
repeats = 3, index = cv.folds)
setwd("~/Desktop/Data Science/Tutorials/Tutorials/Text Analytics with R/Scripts")
library(tidyverse)
library(e1071)
library(caret)
library(quanteda)
library(irlba)
library(randomForest)
library(doSNOW)
# ----------------- Part 1: Intro + Objective -----------------
# load data
data.raw = read.csv("../Data/spam.csv", stringsAsFactors = F, fileEncoding="latin1")
# data preprocessing
data.raw = data.raw[1:2] # keep only the useful columns
colnames(data.raw) = c("Label", "Text") # rename columns
data.raw$Label = as.factor(data.raw$Label)
# check if there are missing values
length(which(!complete.cases(data.raw)))
# distribution of label
prop.table(table(data.raw$Label))
# count the length of each text
data.raw$TextLength = nchar(data.raw$Text)
summary(data.raw$TextLength)
# visualize text length distribution
ggplot(data.raw, aes(x = TextLength, fill = Label)) +
theme_bw() +
geom_histogram(bins = 80) +
labs(y = "Text Count", x = "Length of Text",
title = "Distribution of Text Lengths")
# summary statistics
data.raw %>%
group_by(Label) %>%
summarise("mean" = mean(TextLength),
"median" = median(TextLength),
"sd" = sd(TextLength))
# t-test
with(data.raw, shapiro.test(TextLength[Label == "ham"]))
with(data.raw, shapiro.test(TextLength[Label == "spam"]))
res.ftest <- var.test(TextLength ~ Label, data = data.raw)
res.ftest
res <- t.test(data.raw$TextLength[data.raw$Label=="ham"],
data.raw$TextLength[data.raw$Label=="spam"], var.equal = FALSE)
# conclusion: text length may help with determining spam/ham
# ----------------- Part 2: Train/Test Split -----------------
# train/test split (70%/30% stratified split)
set.seed(32984)
indicies = createDataPartition(data.raw$Label, times = 1, p = 0.7, list = F)
train.data = data.raw[indicies,]
test.data = data.raw[-indicies,]
# verifying that'createDataPartition' preserved the original proportions of labels
prop.table(table(train.data$Label))
prop.table(table(test.data$Label))
# About BOW or DFM:
# BOW(bag-of-words) or DFM(docuemnt frequency matrix) is a common model to convert
# text into dataframes. Tokenize words into columns and let documents correspond
# to rows, where the cells' values represent the frequency of the token in the document
# Considerations: casing, punctuation, numbers, every word, symbols, similar words
# Thus, pre-processing is crucial for text analytics!
# ----------------- Part 3: Preproessing Pipeline + DFM -----------------
# exploring some issues to be handled with pre-processing
train.data$Text[21]
train.data$Text[38]
train.data$Text[357]
# tokenize SMS text messages
train.tokens = tokens(train.data$Text, what = "word",
remove_numbers = T, remove_punct = T,
remove_symbols = T, remove_hyphens = T)
# see examples of the result
train.tokens[[300]]
train.tokens[[357]]
# lower case all tokens
train.tokens = tokens_tolower(train.tokens)
train.tokens[[357]]
# use quanteda's built-in stopword list for English
# NOTE: always inspect stopword lists for applicability to the problem/domain
train.tokens = tokens_select(train.tokens, stopwords(), selection = "remove")
# see examples of the result
train.tokens[[357]]
# perform stemming on the tokens
train.tokens = tokens_wordstem(train.tokens, language = "english")
train.tokens[[357]]
# create a BOW model
train.tokens.dfm = dfm(train.tokens, tolower = F)
train.tokens.df = as.data.frame(train.tokens.dfm)
dim(train.tokens.df)
view(train.tokens.df[1:10, 1:100]) # problem: sparsity & curse of dimensionality
colnames(train.tokens.df[1:30])
# ----------------- Part 4: BOW Model -----------------
# Per best practice, we will leverage cross validation as the basis of the modeling process
# set up a feature data frame with labels
train.tokens.data = cbind(Label = train.data$Label,
train.tokens.df)
# colnames need some preprocessing
names(train.tokens.data[c(146, 148, 235, 238)])
# cleanup column names in an automatic fashion
names(train.tokens.data) = make.names(names(train.tokens.data))
# use caret to create stratified folds for 10-fold CV repeated 3 times
set.seed(48743)
cv.folds = createMultiFolds(train.data$Label, k = 10, times = 3)
cv.cntrl = trainControl(method = "repeatedcv", number = 10,
repeats = 3, index = cv.folds)
